diff --git a/fastchat/model/model_adapter.py b/fastchat/model/model_adapter.py
index d2ac56f..d799cbc 100644
--- a/fastchat/model/model_adapter.py
+++ b/fastchat/model/model_adapter.py
@@ -336,7 +336,7 @@ def get_generate_stream_function(model: torch.nn.Module, model_path: str):
         # Return a curried stream function that loads the right adapter
         # according to the model_name available in this context.  This ensures
         # the right weights are available.
-        @torch.inference_mode()
+        @torch.no_grad()
         def generate_stream_peft(
             model,
             tokenizer,
diff --git a/fastchat/serve/inference.py b/fastchat/serve/inference.py
index 169f086..daa1e63 100644
--- a/fastchat/serve/inference.py
+++ b/fastchat/serve/inference.py
@@ -56,7 +56,7 @@ def prepare_logits_processor(
     return processor_list
 
 
-@torch.inference_mode()
+@torch.no_grad()
 def generate_stream(
     model,
     tokenizer,
diff --git a/fastchat/serve/model_worker.py b/fastchat/serve/model_worker.py
index 54d51cf..646c076 100644
--- a/fastchat/serve/model_worker.py
+++ b/fastchat/serve/model_worker.py
@@ -242,6 +242,8 @@ class ModelWorker(BaseModelWorker):
             self.init_heart_beat()
 
     def generate_stream_gate(self, params):
+        import torch_npu
+        torch_npu.npu.set_device("npu:0")
         self.call_ct += 1
 
         try:
